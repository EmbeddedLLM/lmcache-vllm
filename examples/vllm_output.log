INFO 09-20 00:57:09 api_server.py:520] vLLM API server version 0.6.1.post2
INFO 09-20 00:57:09 api_server.py:521] args: Namespace(host=None, port=8100, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=5000, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.5, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False)
INFO 09-20 00:57:09 api_server.py:163] Multiprocessing frontend to use ipc:///tmp/52739fb9-a456-4e42-ac27-1538309058f2 for IPC Path.
INFO 09-20 00:57:09 api_server.py:176] Started engine process with PID 689347
/root/vllm/vllm/transformers_utils/tokenizer_group/tokenizer_group.py:23: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer_mode "mistral"` to ensure correct encoding and decoding.
  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)
INFO 09-20 00:57:13 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='mistralai/Mistral-7B-Instruct-v0.2', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.2, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
/root/vllm/vllm/transformers_utils/tokenizer_group/tokenizer_group.py:23: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer_mode "mistral"` to ensure correct encoding and decoding.
  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)
DEBUG 09-20 00:57:14 parallel_state.py:973] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.42.20.58:12345 backend=nccl
DEBUG 09-20 00:57:14 parallel_state.py:986] Distributed KV transfer enabled.
DEBUG 09-20 00:57:14 parallel_state.py:989] rank 0 is KV producer.
DEBUG 09-20 00:57:14 parallel_state.py:1005] torch.distributed initialized
DEBUG 09-20 00:57:14 parallel_state.py:1026] _WORLD initialized for rank 0
DEBUG 09-20 00:57:19 client.py:181] Waiting for output from MQLLMEngine.
DEBUG 09-20 00:57:19 parallel_state.py:1118] _TP initialized for rank 0
DEBUG 09-20 00:57:19 parallel_state.py:1138] _PP initialized for rank 0
DEBUG 09-20 00:57:19 parallel_state.py:1143] Disaggregated prefill enabled, create _DISAGG group
DEBUG 09-20 00:57:19 parallel_state.py:1149] Distributed group is [[0, 1]]
DEBUG 09-20 00:57:19 parallel_state.py:1154] _DISAGG initialized for rank 0
INFO 09-20 00:57:19 model_runner.py:1016] Starting to load model mistralai/Mistral-7B-Instruct-v0.2...
INFO 09-20 00:57:19 weight_utils.py:242] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.54it/s]
Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.32it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.27it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.31it/s]

INFO 09-20 00:57:22 model_runner.py:1027] Loading model weights took 13.4966 GB
INFO 09-20 00:57:23 gpu_executor.py:124] # GPU blocks: 12924, # CPU blocks: 2048
INFO 09-20 00:57:24 model_runner.py:1331] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-20 00:57:24 model_runner.py:1335] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
DEBUG 09-20 00:57:29 client.py:181] Waiting for output from MQLLMEngine.
INFO 09-20 00:57:32 model_runner.py:1458] Graph capturing finished in 8 secs.
DEBUG 09-20 00:57:32 engine.py:125] Starting Startup Loop.
DEBUG 09-20 00:57:33 engine.py:127] Starting Engine Loop.
INFO 09-20 00:57:33 api_server.py:229] vLLM to use /tmp/tmpcs3boctr as PROMETHEUS_MULTIPROC_DIR
WARNING 09-20 00:57:33 serving_embedding.py:189] embedding_mode is False. Embedding API will not work.
INFO 09-20 00:57:33 launcher.py:19] Available routes are:
INFO 09-20 00:57:33 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD
INFO 09-20 00:57:33 launcher.py:27] Route: /docs, Methods: GET, HEAD
INFO 09-20 00:57:33 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 09-20 00:57:33 launcher.py:27] Route: /redoc, Methods: GET, HEAD
INFO 09-20 00:57:33 launcher.py:27] Route: /health, Methods: GET
INFO 09-20 00:57:33 launcher.py:27] Route: /tokenize, Methods: POST
INFO 09-20 00:57:33 launcher.py:27] Route: /detokenize, Methods: POST
INFO 09-20 00:57:33 launcher.py:27] Route: /v1/models, Methods: GET
INFO 09-20 00:57:33 launcher.py:27] Route: /version, Methods: GET
INFO 09-20 00:57:33 launcher.py:27] Route: /v1/chat/completions, Methods: POST
INFO 09-20 00:57:33 launcher.py:27] Route: /v1/completions, Methods: POST
INFO 09-20 00:57:33 launcher.py:27] Route: /v1/embeddings, Methods: POST
INFO:     Started server process [689012]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8100 (Press CTRL+C to quit)
INFO:     ::1:51664 - "GET /v1/models HTTP/1.1" 200 OK
INFO 09-20 00:57:39 logger.py:36] Received request chat-4c85cad1bcf142a9b4372793a504f2a8: prompt: '<s> [INST] Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you? [/INST]', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['\n'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [1, 733, 16289, 28793, 22557, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 733, 28748, 16289, 28793], lora_request: None, prompt_adapter_request: None.
INFO 09-20 00:57:39 engine.py:254] Added request chat-4c85cad1bcf142a9b4372793a504f2a8.
DEBUG 09-20 00:57:39 vllm_adapter.py:295] [rank0]: Failed to receive all KVs and hidden states, redo model forwarding.
DEBUG 09-20 00:57:39 model_runner.py:1628] Sending KV caches
DEBUG 09-20 00:57:39 vllm_adapter.py:196] [rank0]: KV send DONE.
INFO 09-20 00:57:39 metrics.py:351] Avg prompt throughput: 81.2 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     ::1:51664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG 09-20 00:57:39 llm_engine.py:1292] Stopping remote worker execution loop.
DEBUG 09-20 00:57:43 client.py:165] Health probe successful.
DEBUG 09-20 00:57:43 llm_engine.py:1292] Stopping remote worker execution loop.
INFO 09-20 00:57:44 logger.py:36] Received request chat-fbe7a4b4a899483489809a0f77d07a1f: prompt: '<s> [INST] Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you? [/INST]', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['\n'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [1, 733, 16289, 28793, 22557, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 733, 28748, 16289, 28793], lora_request: None, prompt_adapter_request: None.
INFO 09-20 00:57:44 engine.py:254] Added request chat-fbe7a4b4a899483489809a0f77d07a1f.
DEBUG 09-20 00:57:44 vllm_adapter.py:295] [rank0]: Failed to receive all KVs and hidden states, redo model forwarding.
DEBUG 09-20 00:57:44 model_runner.py:1628] Sending KV caches
DEBUG 09-20 00:57:44 vllm_adapter.py:196] [rank0]: KV send DONE.
INFO 09-20 00:57:44 metrics.py:351] Avg prompt throughput: 93.1 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     ::1:51668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG 09-20 00:57:44 llm_engine.py:1292] Stopping remote worker execution loop.
INFO:     ::1:40828 - "GET /v1/models HTTP/1.1" 200 OK
INFO 09-20 00:57:46 logger.py:36] Received request chat-de1352e05876472d9ba19d8f03a99710: prompt: '<s> [INST] Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you? [/INST]', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['\n'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [1, 733, 16289, 28793, 22557, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 733, 28748, 16289, 28793], lora_request: None, prompt_adapter_request: None.
INFO 09-20 00:57:46 engine.py:254] Added request chat-de1352e05876472d9ba19d8f03a99710.
DEBUG 09-20 00:57:46 vllm_adapter.py:295] [rank0]: Failed to receive all KVs and hidden states, redo model forwarding.
DEBUG 09-20 00:57:47 model_runner.py:1628] Sending KV caches
DEBUG 09-20 00:57:47 vllm_adapter.py:196] [rank0]: KV send DONE.
INFO:     ::1:40828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG 09-20 00:57:47 llm_engine.py:1292] Stopping remote worker execution loop.
INFO 09-20 00:57:52 logger.py:36] Received request chat-cd7bf08c5cf148ee984412cdd45dd9c0: prompt: '<s> [INST] Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you? [/INST]', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['\n'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [1, 733, 16289, 28793, 22557, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 733, 28748, 16289, 28793], lora_request: None, prompt_adapter_request: None.
INFO 09-20 00:57:52 engine.py:254] Added request chat-cd7bf08c5cf148ee984412cdd45dd9c0.
DEBUG 09-20 00:57:52 vllm_adapter.py:295] [rank0]: Failed to receive all KVs and hidden states, redo model forwarding.
DEBUG 09-20 00:57:52 model_runner.py:1628] Sending KV caches
DEBUG 09-20 00:57:52 vllm_adapter.py:196] [rank0]: KV send DONE.
INFO 09-20 00:57:52 metrics.py:351] Avg prompt throughput: 137.1 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     ::1:40832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG 09-20 00:57:52 llm_engine.py:1292] Stopping remote worker execution loop.
DEBUG 09-20 00:57:53 llm_engine.py:1292] Stopping remote worker execution loop.
DEBUG 09-20 00:57:53 client.py:165] Health probe successful.
INFO:     ::1:40840 - "GET /v1/models HTTP/1.1" 200 OK
INFO 09-20 00:57:55 logger.py:36] Received request chat-c1130f6a44de457c8e3dc871348b286b: prompt: '<s> [INST] Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you? [/INST]', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['\n'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [1, 733, 16289, 28793, 22557, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 733, 28748, 16289, 28793], lora_request: None, prompt_adapter_request: None.
INFO 09-20 00:57:55 engine.py:254] Added request chat-c1130f6a44de457c8e3dc871348b286b.
DEBUG 09-20 00:57:55 vllm_adapter.py:295] [rank0]: Failed to receive all KVs and hidden states, redo model forwarding.
DEBUG 09-20 00:57:56 model_runner.py:1628] Sending KV caches
DEBUG 09-20 00:57:56 vllm_adapter.py:196] [rank0]: KV send DONE.
INFO:     ::1:40840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG 09-20 00:57:56 llm_engine.py:1292] Stopping remote worker execution loop.
INFO 09-20 00:58:01 logger.py:36] Received request chat-7461c77cd6aa48d59fd5dc27534681a4: prompt: '<s> [INST] Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you?Hello how are you? [/INST]', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['\n'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [1, 733, 16289, 28793, 22557, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 16230, 910, 460, 368, 28804, 733, 28748, 16289, 28793], lora_request: None, prompt_adapter_request: None.
INFO 09-20 00:58:01 engine.py:254] Added request chat-7461c77cd6aa48d59fd5dc27534681a4.
DEBUG 09-20 00:58:01 vllm_adapter.py:295] [rank0]: Failed to receive all KVs and hidden states, redo model forwarding.
DEBUG 09-20 00:58:01 model_runner.py:1628] Sending KV caches
DEBUG 09-20 00:58:01 vllm_adapter.py:196] [rank0]: KV send DONE.
INFO 09-20 00:58:01 metrics.py:351] Avg prompt throughput: 111.6 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     ::1:47164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG 09-20 00:58:01 llm_engine.py:1292] Stopping remote worker execution loop.
DEBUG 09-20 00:58:03 llm_engine.py:1292] Stopping remote worker execution loop.
DEBUG 09-20 00:58:03 client.py:165] Health probe successful.
DEBUG 09-20 00:58:04 launcher.py:54] port 8100 is used by process psutil.Process(pid=689012, name='pt_main_thread', status='running', started='00:57:05') launched with command:
DEBUG 09-20 00:58:04 launcher.py:54] python3 -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --port 8100 --max-model-len 5000 --gpu-memory-utilization 0.5
INFO 09-20 00:58:04 launcher.py:57] Shutting down FastAPI HTTP server.
DEBUG 09-20 00:58:04 engine.py:132] Shutting down MQLLMEngine.
DEBUG 09-20 00:58:04 engine.py:134] MQLLMEngine is shut down.
ERROR 09-20 00:58:05 torch_distributed_pipe.py:226] [rank0]: Exception when trying to send tensor([-150886311]), msg: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.42.20.58]:5553
Traceback (most recent call last):
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 221, in send_tensor_wrapper
    self._send_impl(tensor)
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 191, in _send_impl
    self._send_metadata(metadata)
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 155, in _send_metadata
    torch.distributed.send(
  File "/root/miniconda3/envs/vllm-lmcache/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/vllm-lmcache/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 1952, in send
    group.send([tensor], group_dst_rank, tag).wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.42.20.58]:5553
ERROR 09-20 00:58:05 torch_distributed_pipe.py:226] [rank0]: Exception when trying to send tensor([-150886311]), msg: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.42.20.58]:5553
Traceback (most recent call last):
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 221, in send_tensor_wrapper
    self._send_impl(tensor)
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 191, in _send_impl
    self._send_metadata(metadata)
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 155, in _send_metadata
    torch.distributed.send(
  File "/root/miniconda3/envs/vllm-lmcache/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/vllm-lmcache/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 1952, in send
    group.send([tensor], group_dst_rank, tag).wait()
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.42.20.58]:5553
ERROR 09-20 00:58:05 torch_distributed_pipe.py:226] [rank0]: Exception when trying to send tensor([-150886311]), msg: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.42.20.58]:5553
Traceback (most recent call last):
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 221, in send_tensor_wrapper
    self._send_impl(tensor)
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 191, in _send_impl
    self._send_metadata(metadata)
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 155, in _send_metadata
    torch.distributed.send(
  File "/root/miniconda3/envs/vllm-lmcache/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/vllm-lmcache/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 1952, in send
    group.send([tensor], group_dst_rank, tag).wait()
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.42.20.58]:5553
ERROR 09-20 00:58:05 torch_distributed_pipe.py:226] [rank0]: Exception when trying to send tensor([-150886311]), msg: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.42.20.58]:5553
Traceback (most recent call last):
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 221, in send_tensor_wrapper
    self._send_impl(tensor)
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 191, in _send_impl
    self._send_metadata(metadata)
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 155, in _send_metadata
    torch.distributed.send(
  File "/root/miniconda3/envs/vllm-lmcache/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/vllm-lmcache/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 1952, in send
    group.send([tensor], group_dst_rank, tag).wait()
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.42.20.58]:5553
ERROR 09-20 00:58:05 torch_distributed_pipe.py:280] Encountering exception in KV receiving thread
ERROR 09-20 00:58:05 torch_distributed_pipe.py:281] [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.42.20.58]:5553
Exception in thread Thread-4 (drop_select_handler):
Traceback (most recent call last):
  File "/root/miniconda3/envs/vllm-lmcache/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/root/miniconda3/envs/vllm-lmcache/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/root/vllm/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py", line 157, in drop_select_handler
    raise e
  File "/root/vllm/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py", line 125, in drop_select_handler
    roi = self.data_pipe.recv_tensor()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllm/vllm/distributed/kv_transfer/kv_pipe/torch_distributed_pipe.py", line 274, in recv_tensor
    future = self.transport_thread.submit(self._recv_impl)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/vllm-lmcache/lib/python3.12/concurrent/futures/thread.py", line 170, in submit
    raise RuntimeError('cannot schedule new futures after shutdown')
RuntimeError: cannot schedule new futures after shutdown
INFO:     Shutting down
DEBUG 09-20 00:58:06 client.py:168] Shutting down MQLLMEngineClient check health loop.
DEBUG 09-20 00:58:06 client.py:235] Shutting down MQLLMEngineClient output handler.
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
